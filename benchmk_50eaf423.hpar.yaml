Defaults:
  # note, CFS-based location will run slower vs. SCRATCH
  cfs_out: &CFS_OUT  /global/homes/b/balewski/prje/tmp_NyxHydroXXXX/
  cfs_data: &CFS_DATA  /global/homes/b/balewski/prje/superRes-Nyx2022a
  summit_data: &AST153_PROJ  /gpfs/alpine/ast153/proj-shared/balewski/superRes-packed_h5XX/
  summit_out: &AST153_WORLD /gpfs/alpine/world-shared/ast153/balewski/tmp_NyxHydro4kDXXX/

checkpoint_interval/epochs: 50
comment: production config for Super-Resolution
const_local_batch: true
# optional, reduces training samples
#max_glob_samples_per_epoch: 256  # do 1400 or 256=BS32
myId: 50eaf423
num_cpu_workers: 8
num_inp_chan: 1
opt_pytorch: {autotune: true, detect_anomaly: false, zerograd: true}
pred_dump_interval/epochs: 100
text_log_interval/epochs: 1
image_flip_rot: true

facility_conf:
  perlmutter:
    D_LR: {decay/epochs: auto, init: 2.39e-05, reduce: 0.0562, warmup/epochs: 20}
    G_LR: {decay/epochs: auto, init: 9.99e-05, reduce: 0.0288, warmup/epochs: 20}
    batch_size: 4
    base_path:  *CFS_OUT
    #data_path: /pscratch/sd/b/balewski/superRes-packed_h5XXX/
    data_path: *CFS_DATA
    
  summit:
    base_path:  *AST153_WORLD
    data_path:  *AST153_PROJ
    batch_size: 4
    D_LR: missing

  crusher:
    base_path:  *AST153_WORLD
    data_path:  *AST153_PROJ
    D_LR: {decay/epochs: auto, init: 2.39e-05, reduce: 0.0562, warmup/epochs: 20}
    G_LR: {decay/epochs: auto, init: 9.99e-05, reduce: 0.0288, warmup/epochs: 20}
    batch_size: 4

data_conf:
    #field_name: baryon_density
    field_name: dm_density
    zred_ini: z200
    zred_fin: z3

model_conf:
  D:
    conv_block:
      bn: [1, 0, 0, 0, 1, 0, 1]
      filter: [32, 64, 128, 128, 256, 512, 512]
      kernel: [2, 2, 2, 2, 2, 2, 2]
      stride: [2, 2, 2, 2, 2, 1, 1]
    fc_block:
      dims: [64, 64, 32]
      dropFrac: 0.119
    summary_verbosity: 0  # 0=off, 1=layers, 2=torchsummary, 3=both
  G:
    first_cnn_chan: 32
    num_resid_conv: 13
    num_upsamp_bits: 2
    conv_block3: # CNN params, last chan must be 1
       filter: [60, 30, 10,  1]
       kernel: [ 5,  5,  5,  5]
    summary_verbosity: 0  # 0=off, 1=layers, 2=torchsummary, 3=both
    
  comment: hpoA perlmutter 20220212_111627_PST
  tb_model_graph: null
  

qa_hpo_1gpu: {D_param_count: 13648577, G_param_count: 329778, adv_samples_per_sec: 41.54471043913954,
  facility: perlmutter}

train_conf:
  PG_LR: {init: 0.0001}
  pre_epochs: 20
  adv_epochs: 40
  advers_weight: 0.2
  content_weight: 0.8
  early_stop_discr: {G_thres: 0.03, ring_size/epochs: 200}
  fft_weight: 0.3
  msum_weight: 0.06
  perc_warmup/epochs: 150
  pixel_weight: 8.0
  resume: false
  resume_d_weight: ''
  resume_g_weight: ''
  resume_p_weight: ''
  start_epoch: 0
  start_pre_epoch: 0
